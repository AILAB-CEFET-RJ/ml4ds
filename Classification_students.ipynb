{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGCPKYwU4poC"
      },
      "source": [
        "# Predicting beer consumption\n",
        "Online class on Supervised Learning, Wednesday, 29th of October 2025\n",
        "\n",
        "For BIP \"Machine Learning for Data Science\" by Marieke Bouma & Remi ThÃ¼ss, Hanze"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oypXkt7G4poD"
      },
      "source": [
        "## Importing required modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGPWrFcy4poD"
      },
      "outputs": [],
      "source": [
        "# Install packages for the project\n",
        "!pip install pandas numpy seaborn matplotlib scikit-learn scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-aSvQmm4poE"
      },
      "outputs": [],
      "source": [
        "# Import the required building blocks\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myDKsjAT4poE"
      },
      "source": [
        "# Beer consumption as a classification issue\n",
        "Let's try to predict whether \"very little\", \"little\", \"much\" or \"very much\" beer is drunk; A classification.\n",
        "\n",
        "To this end, the data is processed in which the number of Litres is divided into four classes.\n",
        "The limit of the classes is roughly based on the quartile distribution.\n",
        "\n",
        "- I: under 22,000 Litres\n",
        "- II: between 22,000 and 25,000 Litres\n",
        "- III: between 25,000 and 29,000 Litres\n",
        "- IV: More than 29,000 Litres\n",
        "\n",
        "If all goes well, these classes are of comparable size (i.e. somewhat balanced)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejjxioR64poE"
      },
      "source": [
        "## Data analysis and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dht5lREt4poE"
      },
      "outputs": [],
      "source": [
        "# Back to the source\n",
        "df = pd.read_csv('Beerconsumption.csv')\n",
        "df.dropna(inplace=True)\n",
        "df.Date = pd.to_datetime(df.Date)\n",
        "df.Weekend = df.Weekend.astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfOx5p5C4poE"
      },
      "source": [
        "### Making classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jtl-hekH4poE"
      },
      "outputs": [],
      "source": [
        "# Making the classes (this is one method, many exist)\n",
        "df['Class1'] = np.where(df['Litres'] > 29000, 1, 0)\n",
        "df['Class2'] = np.where(df['Litres'] > 25000, 1, 0)\n",
        "df['Class3'] = np.where(df['Litres'] > 22000, 1, 0)\n",
        "df['Class4'] = np.where(df['Litres'] > 10000, 1, 0)\n",
        "\n",
        "df['Class'] = df.Class1 + df.Class2 + df.Class3 + df.Class4\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSub3W_h4poE"
      },
      "outputs": [],
      "source": [
        "# The auxiliary columns can leave again\n",
        "df.drop(columns=['Class1', 'Class2', 'Class3', 'Class4'], inplace=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MKKhm0_4poE"
      },
      "outputs": [],
      "source": [
        "# Just a check of the number of Litres per class\n",
        "perClass = df.groupby('Class').agg({'Litres':['count','sum','mean','std','min','median', 'max']}).round(1)\n",
        "print(perClass)\n",
        "\n",
        "plt.figure(figsize=(10,7))\n",
        "sns.boxplot(x=\"Class\", y=\"Litres\", data=df)\n",
        "plt.xticks(range(0,4),[\"I\", \"II\", \"III\", \"IV\"])\n",
        "plt.title(\"Classes\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cWTplni4poE"
      },
      "source": [
        "### 1. Feature selection\n",
        "Is the Date a useful feature for our goal? Would another be useful? How can you check?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwlxi3mQ4poE"
      },
      "outputs": [],
      "source": [
        "# Let's add the season, because why not\n",
        "df['Season'] = df.Date.dt.month.map({1:1, 2:1, 3:2, 4:2, 5:2, 6:3, 7:3, 8:3, 9:4, 10:4, 11:4, 12:1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rFmcGtI4poE"
      },
      "outputs": [],
      "source": [
        "# Plot the correlation matrix for all features except the date\n",
        "kolomdf = ['Season', 'AvgTemp', 'MinTemp', 'MaxTemp', 'Rainfall_mm', 'Weekend', 'Class']\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "cm = np.corrcoef(df[kolomdf].values.T)\n",
        "sns.set(font_scale=1.5)\n",
        "hm = sns.heatmap(cm,\n",
        "                cbar=True,\n",
        "                annot=True,\n",
        "                square=True,\n",
        "                fmt='.2f',\n",
        "                annot_kws={'size': 15},\n",
        "                yticklabels=kolomdf,\n",
        "                xticklabels=kolomdf\n",
        "                )\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSThxAsb4poE"
      },
      "source": [
        "We'll make a simple model, using only MaxTemp and Weekend as features. AvgTemp and MinTemp are too strongly correlated with MaxTemp, and Rainfall and Season are only very weakly correlated with Class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewWdyrAg4poE"
      },
      "outputs": [],
      "source": [
        "# We'll use as the predictors/features/independent variables MaxTemp and Weekend,\n",
        "# and the class is the value to be predicted (label/independent var)\n",
        "labels = df.loc[:,'Class']\n",
        "data = df.loc[:,['MaxTemp', 'Weekend']]\n",
        "\n",
        "# Insight into the frequency distribution of the classes;\n",
        "# They are roughly balanced (as intended)\n",
        "labels[:].groupby(labels[:]).plot(kind=\"hist\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h52GpWIj4poF"
      },
      "source": [
        "## 2. Baseline models for classification\n",
        "Code a baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7vCUMTu4poF"
      },
      "outputs": [],
      "source": [
        "# splits the dataset in a training set and a test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.25, random_state=0)\n",
        "\n",
        "\n",
        "# give the size of the training and the test set again\n",
        "print('shape of training set: ', X_train.shape)\n",
        "print('shape of test set:     ', X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmdDCrBs4poF"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 2.1 Choose a baseline classification model, fill in the blank\n",
        "baseline_1 = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "baseline_1.fit(X_train, y_train)\n",
        "\n",
        "y_pred = baseline_1.predict(X_test)\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy:', acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ucm6wG734poF"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 2.2 Choose a different baseline classification model, fill in the blank\n",
        "baseline_2 = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "baseline_2.fit(X_train, y_train)\n",
        "\n",
        "y_pred = baseline_2.predict(X_test)\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy:', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RicToYA4poF"
      },
      "source": [
        "#### Question: how do these models compare? Is there a difference in accuracy? Why do you think this is?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mxo-HsxR4poF"
      },
      "source": [
        "## 3/4. Tree-based models\n",
        "Code a decision tree and random forest below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhhAhago4poF"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Fill in the blank with a decision tree model\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = dt_model.predict(X_test)\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy:', acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JN-zRKoa4poF"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Fill in the blank with a random forest model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print('Accuracy:', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxGPC17b4poF"
      },
      "source": [
        "#### Challenge: visualise the decision making process\n",
        "There exist some cool libraries to visualise the decision making process in a tree-based model. Try find these libraries and apply them to one of your models!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKpQ_HZR4poF"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j46B4LMH4poF"
      },
      "source": [
        "## 5. Neural networks\n",
        "Complete the code for a neural network for the beer consumption classification issue!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHgRGBVf4swx"
      },
      "outputs": [],
      "source": [
        "# # The neural network expects the classes 0 1 2 3 instead of 1 2 3 4.\n",
        "# y_train = y_train - 1\n",
        "# y_test = y_test - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LO76KkG74poF"
      },
      "outputs": [],
      "source": [
        "# # Build the model\n",
        "# neural_net = Sequential([\n",
        "#     Input(shape=(2,)),\n",
        "#     Dense(64, activation='relu'),\n",
        "#     Dense(32, activation='relu'),\n",
        "#     Dense(4, activation='softmax')\n",
        "# ])\n",
        "\n",
        "# # Compile\n",
        "# neural_net.compile(\n",
        "#     optimizer='adam',\n",
        "#     loss='sparse_categorical_crossentropy',\n",
        "#     metrics=['accuracy', 'f1_score']\n",
        "# )\n",
        "\n",
        "# # Train\n",
        "# neural_net.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# # Evaluate the model\n",
        "# loss, accuracy, f1 = neural_net.evaluate(X_test[:], y_test)\n",
        "# print(f\"Test Accuracy: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K37samc4poF"
      },
      "source": [
        "#### More on neural networks in a later class!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkPIXipU4poF"
      },
      "source": [
        "## 6. Metrics (Confusion matrix)\n",
        "Below you can find some standard code to make and visualise a confusion matrix. The score that is usually used is called the F1-score. You will learn more on model evaluation in a later class, but here's a taste!\n",
        "\n",
        "Use the \"classification report\" code to calculate the F1-scores of all of your models*. How well do they perform? Do the F1-score and accuracy differ?\n",
        "\n",
        "*excluding the neural network, because it already shows the F1-score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngTrVOc34poF"
      },
      "outputs": [],
      "source": [
        "confmat = confusion_matrix(y_true = y_test, y_pred = y_pred)\n",
        "\n",
        "#image for confusion matrix\n",
        "fig, ax = plt.subplots(figsize = (5, 5))\n",
        "ax.matshow(confmat, alpha = 0.3)\n",
        "for i in range(confmat.shape[0]):\n",
        "    for j in range(confmat.shape[1]):\n",
        "        ax.text(x=j, y=i,\n",
        "                s=confmat[i, j],\n",
        "                fontsize=18,\n",
        "                va='center', ha='center')\n",
        "ticks = labels.unique()\n",
        "ticks.sort()\n",
        "ax.set_xticks(np.arange(confmat.shape[0]), labels=ticks)\n",
        "ax.set_yticks(np.arange(confmat.shape[1]), labels=ticks)\n",
        "plt.xlabel('predicted label')\n",
        "plt.ylabel('true label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7rIWAQs4poF"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iBdbMtg4poF"
      },
      "outputs": [],
      "source": [
        "# [your code to compare the accuracy and f1-score for each model here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7up65Dj4poF"
      },
      "source": [
        "[your analysis here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zA8KiVq4poF"
      },
      "source": [
        "## 7. A critical look at the data\n",
        "We've seen that the data is somewhat balanced when it comes to our 4 classes. However, we get rather bad results for classification. Perhaps a four-class classification task is simply not very useful with such little data.\n",
        "\n",
        "#### Binary classification\n",
        "Rerun the entire pipeline* with only two classes. Consider with your group where to put the threshold for the classification. How do the performances change?\n",
        "\n",
        "*from preprocessing and visualisation, to training the models, to measuring the model performances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0mb-jUK4poF"
      },
      "outputs": [],
      "source": [
        "# [your code and/or markdown here]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
